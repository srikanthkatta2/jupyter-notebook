{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22ab5b1-a93a-431f-987b-fa2dac5d311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ad1c8e-8cac-4a08-afa0-f3fd7d459638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3c14f6-f49f-43d9-885d-7d901cce0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 35\n",
      " \n",
      " \n",
      " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
      "Acknowledgements  \n",
      "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
      "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
      "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
      "2014‐34. \n",
      " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
      " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
      " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
      " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
      " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
      " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
      " \n",
      "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
      " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
      " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
      " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
      "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
      " beginning  of the project and their \n",
      "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
      "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
      "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
      "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
      "understanding  actual ground conditions.  \n",
      " \n",
      "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
      "and anticipate  the work's usefulness  for the intended purpose. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Creating a pdf file object\n",
    "pdf = open(\"file1pdf.pdf\",\"rb\")\n",
    "#creating pdf reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "#checking number of pages in a pdf file\n",
    "print(\"Number of pages:\",len(pdf_reader.pages))\n",
    "#creating a page object\n",
    "page = pdf_reader.pages[1]\n",
    "#finally extracting text from the page\n",
    "print(page.extract_text())\n",
    "#closing the pdf file\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19134560-87ce-4345-a11c-f5d3afb56ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, urllib , nltk\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce31a36-4fc1-4f67-8f2c-0f7d32c5d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the PDF\n",
    "wFile = urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
    "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a71a9922-867d-44e5-8bc7-808b4243107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting page 2 of the docuemnt\n",
    "pageObj = pdfreader.pages[2]\n",
    "page2 = pageObj.extract_text()\n",
    "#Cleaning the text\n",
    "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
    "tokens = word_tokenize(page2)\n",
    "stop_words = stopwords.words('english')\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceab0219-1fcd-49ea-8e42-2b2ff90fba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
     ]
    }
   ],
   "source": [
    "name_list = list()\n",
    "check =  ['Mr.', 'Mrs.', 'Ms.']\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token.startswith(tuple(check)) and idx < (len(tokens)-1):\n",
    "        name = token + tokens[idx+1] + ' ' +  tokens[idx+2]\n",
    "        name_list.append(name)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "921eb451-726a-4fe6-b522-526b65597c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "# install docx library\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5578b236-160c-49d2-bf8c-683cb3611996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a713a9-0718-4691-b546-5ec0e7902969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a word file object\n",
    "doc = open(\"BDA Minor 1 Exam Lab program.docx\",\"rb\")\n",
    "#creating word reader object\n",
    "document = docx.Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "492765ad-afd0-4600-9c3e-211837edc564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data AnalyticsMinor- 1(Case Study)  Lab programsQuestion Set By using the below array perform the following  Numpy operations:         import numpy as np         arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])   Find the shape of the array  Reshape the array into a 1D array    Extract the second column of the array Compute the sum of all elements in the array    Find the maximum value in the array    Calculate the mean of each row    Multiply the array by a scalar value of 2  Perform matrix multiplication of the array with its transpose    Save the final array to a .npy file.       By using the below dataset in a CSV file named employees.csv,  Perform the           following operations using Pandas:  Load the dataset into a DataFrame.Add a new column Bonus which is 10% of the Salary.  Filter the DataFrame to show only employees in the IT department.Group the data by Department and calculate the average Salary for each department.  Sort the DataFrame by Age in ascending order.  Save the final DataFrame to a new CSV file named updated_employees.csv.  3. By using the below dataset solve the problems “a company wants to analyze         Employee data to identify salary trends and performance”Load the dataset into a Pandas DataFrame.How many employees are in each department?What is the average salary per department?Who is the highest-paid employee?Find employees who have been in the company for more than 3 years.Identify employees with a performance score above 85.Convert the \"Joining Date\" column to datetime format and calculate years of experience.Sort the employees by salary in descending order. 4. Do the analysis by using Numpy and Pandas for the Mini-Project, by using of Boston           Housing data set. 5. Create Basic Plots using Matplot Library using any Data Frame. 6. Create Basic Plots using SeaBorn Library using any Data Frame. 7. Write a detailed step-by-step guide for installing, configuring, and running Hadoop in       Windows. 8. Implement word count / Frequency program using Map Reduce in Hadoop. 9. Implement a Map Reduce Program that process a data set.10. Use Any Data set to perform the following Operations.Load and Explore the Data Data Cleaning and Preprocessing Data Analysis Data Visualization \n"
     ]
    }
   ],
   "source": [
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "#to see the output call docu\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12ad3ab1-b832-4391-934c-44ac629f03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the paragraph 0 is ：Big Data Analytics\n",
      "\n",
      "The content of the paragraph 1 is ：Minor- 1(Case Study)  Lab programs\n",
      "\n",
      "The content of the paragraph 2 is ：Question Set\n",
      "\n",
      "The content of the paragraph 3 is ： By using the below array perform the following  Numpy operations:\n",
      "\n",
      "The content of the paragraph 4 is ：         import numpy as np\n",
      "\n",
      "The content of the paragraph 5 is ：         arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      "\n",
      "The content of the paragraph 6 is ：   Find the shape of the array  \n",
      "\n",
      "The content of the paragraph 7 is ：Reshape the array into a 1D array \n",
      "\n",
      "The content of the paragraph 8 is ：   Extract the second column of the array \n",
      "\n",
      "The content of the paragraph 9 is ：Compute the sum of all elements in the array \n",
      "\n",
      "The content of the paragraph 10 is ：   Find the maximum value in the array \n",
      "\n",
      "The content of the paragraph 11 is ：   Calculate the mean of each row \n",
      "\n",
      "The content of the paragraph 12 is ：   Multiply the array by a scalar value of 2  \n",
      "\n",
      "The content of the paragraph 13 is ：Perform matrix multiplication of the array with its transpose \n",
      "\n",
      "The content of the paragraph 14 is ：   Save the final array to a .npy file.   \n",
      "\n",
      "The content of the paragraph 15 is ：    \n",
      "\n",
      "The content of the paragraph 16 is ：By using the below dataset in a CSV file named employees.csv,  Perform the    \n",
      "\n",
      "The content of the paragraph 17 is ：       following operations using Pandas:\n",
      "\n",
      "The content of the paragraph 18 is ：  Load the dataset into a DataFrame.\n",
      "\n",
      "The content of the paragraph 19 is ：Add a new column Bonus which is 10% of the Salary.\n",
      "\n",
      "The content of the paragraph 20 is ：  Filter the DataFrame to show only employees in the IT department.\n",
      "\n",
      "The content of the paragraph 21 is ：Group the data by Department and calculate the average Salary for each department.\n",
      "\n",
      "The content of the paragraph 22 is ：  Sort the DataFrame by Age in ascending order.\n",
      "\n",
      "The content of the paragraph 23 is ：  Save the final DataFrame to a new CSV file named updated_employees.csv.\n",
      "\n",
      "The content of the paragraph 24 is ：\n",
      "\n",
      "The content of the paragraph 25 is ：  3. By using the below dataset solve the problems “a company wants to analyze  \n",
      "\n",
      "The content of the paragraph 26 is ：       Employee data to identify salary trends and performance”\n",
      "\n",
      "The content of the paragraph 27 is ：Load the dataset into a Pandas DataFrame.\n",
      "\n",
      "The content of the paragraph 28 is ：How many employees are in each department?\n",
      "\n",
      "The content of the paragraph 29 is ：What is the average salary per department?\n",
      "\n",
      "The content of the paragraph 30 is ：Who is the highest-paid employee?\n",
      "\n",
      "The content of the paragraph 31 is ：Find employees who have been in the company for more than 3 years.\n",
      "\n",
      "The content of the paragraph 32 is ：Identify employees with a performance score above 85.\n",
      "\n",
      "The content of the paragraph 33 is ：Convert the \"Joining Date\" column to datetime format and calculate years of experience.\n",
      "\n",
      "The content of the paragraph 34 is ：Sort the employees by salary in descending order.\n",
      "\n",
      "The content of the paragraph 35 is ： 4. Do the analysis by using Numpy and Pandas for the Mini-Project, by using of Boston     \n",
      "\n",
      "The content of the paragraph 36 is ：      Housing data set.\n",
      "\n",
      "The content of the paragraph 37 is ： 5. Create Basic Plots using Matplot Library using any Data Frame.\n",
      "\n",
      "The content of the paragraph 38 is ： 6. Create Basic Plots using SeaBorn Library using any Data Frame.\n",
      "\n",
      "The content of the paragraph 39 is ： 7. Write a detailed step-by-step guide for installing, configuring, and running Hadoop in  \n",
      "\n",
      "The content of the paragraph 40 is ：     Windows.\n",
      "\n",
      "The content of the paragraph 41 is ： 8. Implement word count / Frequency program using Map Reduce in Hadoop.\n",
      "\n",
      "The content of the paragraph 42 is ： 9. Implement a Map Reduce Program that process a data set.\n",
      "\n",
      "The content of the paragraph 43 is ：10. Use Any Data set to perform the following Operations.\n",
      "\n",
      "The content of the paragraph 44 is ：Load and Explore the Data \n",
      "\n",
      "The content of the paragraph 45 is ：Data Cleaning and Preprocessing \n",
      "\n",
      "The content of the paragraph 46 is ：Data Analysis \n",
      "\n",
      "The content of the paragraph 47 is ：Data Visualization \n",
      "\n",
      "The content of the paragraph 48 is ：\n",
      "\n",
      "The content of the paragraph 49 is ：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  output paragraph number and paragraph content \n",
    "for i in range(len(document.paragraphs)):\n",
    "    print(\"The content of the paragraph \"+ str(i)+\" is ：\" + document.paragraphs[i].text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d072bf9-c9f2-414a-b8a4-4cb419a2397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\srikanth\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "# Install and import all the necessary libraries\n",
    "\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47c132fe-6e6a-493f-b5f3-4b78c88fbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5485c3a0-1e3d-4228-9f8a-7dfb37472577",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f617028e-be7b-495b-93fe-b3a09a0e9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Natural language processing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
      "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"bb8cccbb-931f-4281-aaf8-3aff8c9effb8\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1274942014,\"wgRevisionId\":1274942014,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"All accuracy disputes\",\"Accuracy disputes from December 2013\",\"Harv and Sfn no-target errors\",\"CS1 errors: periodical ignored\",\"CS1 maint: location\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from May 2024\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from May 2024\",\"Commons category link from Wikidata\",\n",
      "\"Natural language processing\",\"Computational fields of study\",\"Computational linguistics\",\"Speech recognition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Natural_language_processing\",\"wgRelevantArticleId\":21652,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\n",
      "\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q30642\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\n",
      "\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visu\n"
     ]
    }
   ],
   "source": [
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "# Print few lines\n",
    "print (strhtm[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe8969-7472-4ed2-9b0a-e2392a5bcd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
